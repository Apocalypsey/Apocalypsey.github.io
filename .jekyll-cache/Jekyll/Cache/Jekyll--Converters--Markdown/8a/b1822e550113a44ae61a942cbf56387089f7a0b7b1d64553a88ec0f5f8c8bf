I"Ü<p>æœ€è¿‘æ¥æ‰‹äº†åŒäº‹çš„ä¸€éƒ¨åˆ†ä»£ç ï¼Œæ˜¯å¯¹å„ç§ç‰¹å¾çš„ç¦»çº¿è®¡ç®—å’Œå†™å…¥redisï¼Œæˆ‘è¿›è¡Œç®¡ç†å’ŒäºŒæ¬¡å¼€å‘ï¼Œåœ¨ç³»ç»Ÿä¸­åšæˆå¤©æä»»åŠ¡æ¯å¤©å®šæ—¶è·‘ã€‚é¡¹ç›®éƒ¨ç½²å®Œè·‘èµ·æ¥åå‘ç°è¿™ä¸ªä»»åŠ¡è·‘çš„æ—¶é—´å®åœ¨æ˜¯å¤ªä¹…å¤ªä¹…äº†ï¼Œé‚£æ²¡åŠæ³•ï¼Œä¼˜åŒ–å‘—ã€‚</p>

<h3 id="åŸå§‹ç‰ˆæœ¬">åŸå§‹ç‰ˆæœ¬</h3>
<pre><code class="language-Scala">userWorkDF.toJSON.foreach { record =&gt;
    val recordJson = JSON.parseObject(record)
    val uid = recordJson.getString("uid")
    val work_id = recordJson.getString("work_id")
    val uid_key = date_key + '_' + uid
    val jedis = BDWGRedisUtils.pool.getResource
    jedis.auth(conf.getString("redis_bdwg.password"))
    jedis.sadd(date_key, uid_key)
    var finalStr = ""
    val featureMap = new util.HashMap[String,String]
    if(jedis.hexists(uid_key, work_id)) {
        val existStr = jedis.hget(uid_key, work_id)
        val existFeature = JSON.parseObject(existStr)
        for (elem &lt;- columnList) {
            val value = recordJson.getDouble(elem)
            val key = columnMap.get(elem)
            if (value &gt; 0.0) {
                featureMap.put(key, value.toString)
            }
        }
        val keys = existFeature.keySet().iterator()
        while (keys.hasNext) {
            val key = keys.next()
            featureMap.put(key, existFeature.getString(key))
        }
    } else {
        for (elem &lt;- columnList) {
            val value = recordJson.getDouble(elem)
            val key = columnMap.get(elem)
            if (value &gt; 0.0) {
                featureMap.put(key, value.toString)
            }
        }
    }
    finalStr = JSON.toJSONString(featureMap, SerializerFeature.EMPTY: _*)
    jedis.hset(uid_key, work_id, finalStr)
    jedis.expire(uid_key, conf.getInt("redis_bdwg.key.expire"))
    jedis.expire(date_key, conf.getInt("redis_bdwg.key.expire"))
    BDWGRedisUtils.pool.returnResource(jedis)
}
</code></pre>
<p>åˆ†æcodeåå‘ç°ï¼Œjedisè¿™ä¸ªå¯¹è±¡è¢«é‡å¤åˆå§‹åŒ–ä½¿ç”¨ï¼Œæ¯æ¬¡éå†éƒ½éœ€è¦åˆ›å»ºä¸€ä¸ªjediså¯¹è±¡ï¼Œç„¶åå†è¿›è¡Œå¯†ç éªŒè¯ï¼Œæœ€åè¢«å›æ”¶æ‰.é‚£ä¹ˆèƒ½ä¸èƒ½æŠŠå®ƒæ”¾åˆ°å¾ªç¯å¤–é¢ï¼Œä¸”äº‹å…ˆæ„é€ å¥½uid_keyçš„Mapå’Œuid_workid_finalStrçš„Mapå‘¢ï¼Œå’±ä»¬åŠ¨æ‰‹è¯•è¯•çœ‹</p>

<h3 id="version_1">version_1</h3>
<pre><code class="language-Scala">val jedis = BDWGRedisUtils.pool.getResource
jedis.auth(conf.getString("redis_bdwg.password"))
val userSet = new util.HashMap[String,String]
val uid_work_id_Feature = new util.HashMap[String, util.HashMap[String,String]]
userWorkDF.toJSON.foreach { record =&gt;
    val recordJson = JSON.parseObject(record)
    //do something to build userSet and uid_work_id_Feature
}
userSet.foreach{ uid_key =&gt; 
    jedis.sadd(uid_key, kmp)
    jedis.hset(uid_key, uid_work_id_Feature.get(uid_key))
}
</code></pre>
<p>codeç¼–å†™å®Œæ¯•ï¼Œä¸Šå»æ‰§è¡Œã€‚ã€‚
å®å’šï¼Œå¼¹å‡ºæ¥ä¸€ä¸ªFailã€‚ã€‚å¥½å§ï¼Œæ‹‰ä¸‹æ—¥å¿—æ¥çœ‹çœ‹å“ªé‡Œå‡ºé”™äº†<br />
org.apache.spark.SparkException: Task not serializable 
Claused by: java.io.NotSerializableException: com.redis.RedisClient<br />
æŸ¥äº†ä¸€ä¸‹Sparkçš„è¿è¡Œå›¾å¦‚ä¸‹
<img src="/img/spark.jpg" alt="avatar" />
jediså¯¹è±¡åœ¨Driverä¸­åˆå§‹åŒ–ï¼ŒuserWorkDF.toJSON.foreachä¸­çš„ä»£ç å—ä¼šè¢«åˆ†å‰²åˆ°å„ä¸ªWorker Nodeé‡Œé¢å»æ‰§è¡Œï¼Œè‹¥Worker Nodeéœ€è¦ä½¿ç”¨åˆ°Driverä¸­çš„codeï¼Œåˆ™éœ€è¦å…ˆè¿›è¡Œåºåˆ—åŒ–ï¼Œæ¯ä¸ªWorker Nodeæ‹¿åˆ°åºåˆ—åŒ–åçš„å˜é‡ï¼Œåœ¨è¿›è¡Œååºåˆ—åŒ–æˆå¯¹è±¡è¿›è¡Œä½¿ç”¨ã€‚é‚£ä¹ˆé—®é¢˜æ¥æ¥ï¼Œjediså¯¹è±¡èƒ½å¦è¢«åºåˆ—åŒ–å‘¢ï¼Œç­”æ¡ˆæ˜¯ä¸èƒ½çš„ï¼Œæˆ‘ä»¬æ¥çœ‹æºç :</p>
<pre><code class="language-Java">public class Jedis extends BinaryJedis implements JedisCommands, MultiKeyCommands, AdvancedJedisCommands, ScriptingCommands, BasicCommands, ClusterCommands, SentinelCommands {
    //something
}
</code></pre>
<p>ç”±æ­¤å¯è§ï¼ŒJedisç±»æ²¡æœ‰å®ç°Serializableæ¥å£ï¼Œä¸èƒ½è¢«åºåˆ—åŒ–ã€‚å†ç”±æ­¤Sparkè¿è¡Œå›¾å¯ä»¥å¾—å‡ºï¼Œè‹¥æˆ‘ä»¬æƒ³åœ¨foreachä¸­å¾€å®ç°å®šä¹‰å¥½çš„HashMapæ’å…¥æ•°æ®ï¼Œç­‰foreachç»“æŸåï¼Œæ’å…¥çš„æ•°æ®å¹¶ä¸èƒ½å¸¦å‡ºforeachä»£ç å—ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºHashMapåœ¨Driverä¸­åˆå§‹åŒ–ï¼Œåºåˆ—åŒ–åå‘é€åˆ°å„ä¸ªWorkerè¿›è¡Œååºåˆ—åŒ–ï¼Œè¿™ä¸ªæ˜¯Mapçš„ä¸€ä¸ªå‰¯æœ¬ï¼Œå¾€Mapä¸­æ’å…¥æ•°æ®ï¼Œä¹Ÿæ˜¯åœ¨å‰¯æœ¬ä¸­æ’å…¥ï¼Œå¹¶ä¸èƒ½åœ¨åŸå§‹çš„å¯¹è±¡ä¸­æ’å…¥ï¼Œæ‰€ä»¥ç­‰åˆ°foreachç»“æŸåè¿›è¡Œreduceæ“ä½œï¼Œå„ä¸ªWorkerä¸­çš„å‰¯æœ¬ä¸èƒ½å¤Ÿè¢«å¸¦å‡ºæ¥ï¼Œé™¤éæœ‰reduceï¼Œä½†æ˜¯reduceä¹Ÿæ˜¯ä¸€ä»¶éå¸¸è€—æ—¶çš„å·¥ä½œï¼Œè‹¥æ‰§è¡Œreduceï¼Œèƒ½å¦æœ‰æ€§èƒ½ä¸Šçš„æå‡ï¼Œè¿˜æœ‰å¾…å•†æ¦·ã€‚<br />
ç»§ç»­æ€è€ƒï¼Œè¿˜æœ‰æ²¡æœ‰ä»€ä¹ˆèƒ½å¤Ÿä¼˜åŒ–ï¼Œæé«˜æ€§èƒ½çš„ã€‚æœ‰ï½Pipeline+Partition</p>
<h3 id="version_2-pipelinepartitionä¼˜åŒ–">version_2 Pipeline+Partitionä¼˜åŒ–</h3>
<p>åºŸè¯ä¸å¤šè¯´ï¼Œå…ˆä¸ŠCode</p>
<pre><code class="language-Scala">pskDF.toJSON.foreachPartition(iter =&gt; {
    val jedisp = BDWGRedisUtils.pool.getResource
    jedisp.auth(conf.getString("redis_bdwg.password"))
    var pip_count = 0
    val pipe = jedisp.pipelined()
    while(iter.hasNext) {
        val record = iter.next()
        val recordJson = JSON.parseObject(record)
        val uid = recordJson.getString("uid")
        val work_id = recordJson.getString("work_id")
        val uid_key = date_key + '_' + uid
        if (!Strings.isNullOrEmpty(uid) &amp;&amp; !uid.equalsIgnoreCase("null")) {
            var finalStr = ""
            val featureMap = new mutable.HashMap[String, String]()
            for (elem &lt;- columnList) {
                val value = recordJson.getDouble(elem)
                val key = columnMap.get(elem)
                if (value != null &amp;&amp; value &gt; 0.0) {
                    featureMap.put(key, value.toString)
                }
            }
            finalStr = JSON.toJSONString(featureMap, SerializerFeature.EMPTY: _*)
            pipe.hset(uid_key, work_id, finalStr)
            pipe.sadd(date_key, uid_key)
            pipe.expire(uid_key, jedisExpire)
            pipe.expire(date_key, jedisExpire)
            pip_count += 4
            if (pip_count &gt;= pipeCount) {
                pipe.sync()
                pip_count = 0
            }
        }
    }
    if (pip_count &gt; 0) {
        pipe.sync()
    }
    BDWGRedisUtils.pool.returnResource(jedisp)
})
</code></pre>
<p>Pipelineå¯¹äºæ€§èƒ½çš„æå‡è‡ªç„¶ä¸ç”¨å¤šè¯´ï¼Œä¸ä½¿ç”¨pipelineçš„æƒ…å†µä¸‹ï¼Œredisæ“ä½œéœ€è¦ä¸€æ¡æ¡çš„æ‰§è¡Œï¼Œredisçš„setæ“ä½œå¾ˆå¿«ï¼Œä½†æ˜¯ç½‘ç»œä¸Šçš„æ¶ˆè€—åˆ™æ˜¯éå¸¸æƒŠäººçš„ï¼Œç”¨pipelineå¯ä»¥å°†å¤šæ¡redisæ“ä½œè®°å½•ä¸‹æ¥ï¼Œä¸€å¹¶å‘é€åˆ°redisæœåŠ¡å™¨ï¼ŒèŠ‚çœä¸­é—´çš„ç½‘ç»œæ¶ˆè€—ã€‚è€Œä¸”æˆ‘ä»¬çš„è¿™ä¸ªä¾‹å­ï¼Œå…¨æ˜¯setå’Œexpireæ“ä½œï¼Œå¹¶ä¸ä¼šäº§ç”Ÿå¤šçº¿ç¨‹çš„é—®é¢˜ï¼Œå› æ­¤pipelineæ˜¯éå¸¸åˆé€‚çš„ã€‚
pipelineè§£é‡Šäº†ï¼Œé‚£partitionå‘¢ï¼Ÿpartitionæ€ä¹ˆè§£é‡Šï¼Ÿå’±ä»¬æ¥çœ‹foreachå’Œforeachpartitionçš„æºç </p>
<pre><code class="language-Scala">def foreach(f: T =&gt; Unit): Unit = withScope {
    val cleanF = sc.clean(f)
    sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF))
}

def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withScope {
    val cleanF = sc.clean(f)
    sc.runJob(this, (iter: Iterator[T]) =&gt; cleanF(iter))
}
</code></pre>
<p>ç»“åˆä¸Šé¢version2çš„codeï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œä½¿ç”¨foreachï¼Œä¼šåœ¨å…¶å†…éƒ¨ç”¨è¿­ä»£å™¨è¿›è¡Œä»£ç å¿«çš„å±‚å±‚æ‰§è¡Œï¼Œè‹¥åœ¨ä»£ç å—ä¸­æœ‰å¯¹è±¡çš„ç”Ÿæˆå’Œé”€æ¯ç­‰æ“ä½œï¼Œæ˜¯æ¯”è¾ƒè€—æ—¶çš„ï¼Œè€Œç”¨foreachPartitionå‘¢ï¼Œä¼ å…¥çš„å‚æ•°æ˜¯ä¸€ä¸ªè¿­ä»£å™¨ï¼Œæ¯”è¾ƒè€—èµ„æºçš„æ“ä½œåœ¨å¾ªç¯çš„å¤–é¢æ‰§è¡Œï¼Œæ€§èƒ½ä¼šæé«˜å¾ˆå¤šã€‚
<br />
è‡³æ­¤ï¼Œè¿™ä¸ªé—®é¢˜å°±ç®—æ˜¯æ¯”è¾ƒå®Œç¾çš„è§£å†³äº†ã€‚</p>
<h3 id="ç»“è®º">ç»“è®º</h3>
<ol>
  <li>èƒ½å¤Ÿç”¨foreachPartitionå’ŒmapPartitionæ¥ä»£æ›¿foreachå’Œmapçš„å°½é‡ä½¿ç”¨foreachPartitionã€mapPartition</li>
  <li>åœ¨ä¸å½±å“ä¸€è‡´æ€§çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿç”¨pipelineçš„å°½é‡ä½¿ç”¨pipelineï¼Œä½†æ˜¯ä½¿ç”¨pipelineä¹Ÿè¦æ³¨æ„ï¼Œä¸€æ¬¡æ€§ä¸èƒ½æäº¤å¤ªå¤šï¼Œéœ€è¦ç”¨å˜é‡è¿›è¡Œæ§åˆ¶</li>
</ol>
:ET